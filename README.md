# Neural_network
The simplest definition of neural network mostly referred to as Artificial neural network is:
        "A computing system made up of a simple, highly interconnected processing elements called as neurons which process information by their dynamic state response to external inputs."

Or you can also think ANN as a computational model that is inspired by the way Biological neural networks in the human brain process information.
ANNs are processing devices (algorithms or actual hardware) that are loosely modeled after the neuronal structure of the mamalian cerebral cortex but on much smaller scales. A large ANN might have hundreds or thousands of processor units, whereas a mamalian brain has billions of neurons with a corresponding increase in magnitude of their overall interaction and emergent behavior. Although ANN researchers are generally not concerned with whether their networks accurately resemble biological systems, some have. 

Basics of Neural Network

Neural networks are typically organised in layers. Layers are made up of a number of interconnected "nodes" which contains actication function. Patterns are presented to a network via input layer which communicates to one or more hidden layers where the actual processing is done via system of weighted 'connections'

Most ANNs contain some form of 'learning rule' which modifies the weights of the connections according to the input patterns that it is presented with. In a sense, ANNs learn by example as do their biological counterparts; a child learns to recognize dogs from examples of dogs.

Although there are many different kinds of learning rules used by neural networks, this demonstration is concerned only with one; the delta rule. The delta rule is often utilized by the most common class of ANNs called 'backpropagational neural networks' (BPNNs). Backpropagation is an abbreviation for the backwards propagation of error. 
